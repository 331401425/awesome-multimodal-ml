# Reading List for Topics in Multimodal Machine Learning
By [Paul Pu Liang](http://www.cs.cmu.edu/~pliang/) (pliang@cs.cmu.edu), [Machine Learning Department](http://www.ml.cmu.edu/) and [Language Technologies Institute](https://www.lti.cs.cmu.edu/), [CMU](https://www.cmu.edu/). If there are any areas, papers, and datasets I missed, please let me know!

# Research Papers

## Survey Papers

Multimodal Machine Learning: A Survey and Taxonomy https://arxiv.org/abs/1705.09406

## Core Areas

### Multimodal Fusion

### Multimodal Alignment

On Deep Multi-View Representation Learning, ICML 2015 [paper](http://proceedings.mlr.press/v37/wangb15.pdf)

### Multimodal Translation

### Co-learning

### Graphs

### Knowledge Bases

### Intepretable Learning

### Transfer Learning

### Generative Learning

### Semi-supervised Learning

### Self-supervised Learning

### Unsupervised Learning

## Applications

### Language and Visual QA

VQA: Visual Question Answering, ICCV 2015 [paper](https://arxiv.org/abs/1505.00468) [code](https://visualqa.org/)

Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding, EMNLP 2016 [paper](https://arxiv.org/abs/1606.01847) [code](https://github.com/akirafukui/vqa-mcb)

### Language Grounding in QA

#### Datasets

#### Papers

### Language Grouding in Navigation

Embodied Question Answering, CVPR 2018 [paper](https://arxiv.org/abs/1711.11543) [code](https://embodiedqa.org/)

### Multi-agent Communication

[The Emergence of Compositional Structures in Perceptually Grounded Language Games](https://www.cs.utexas.edu/~kuipers/readings/Vogt-aij-05.pdf), AI 2005

[Learning to communicate with deep multi-agent reinforcement learning](https://arxiv.org/pdf/1605.06676.pdf), NIPS 2016.

[Learning multiagent communication with backpropagation](http://papers.nips.cc/paper/6398-learning-multiagent-communication-with-backpropagation.pdf), NIPS 2016.

[Multi-agent cooperation and the emergence of (natural) language](https://arxiv.org/abs/1612.07182), ICLR 2017

[Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning](https://arxiv.org/pdf/1703.06585.pdf), ICCV 2017 [code](https://github.com/batra-mlp-lab/visdial-rl)

[Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog](https://arxiv.org/abs/1706.08502), EMNLP 2017 [code1](https://github.com/batra-mlp-lab/lang-emerge) [code2](https://github.com/kdexd/lang-emerge-parlai)

[Emergence of Grounded Compositional Language in Multi-Agent Populations](https://arxiv.org/pdf/1703.04908.pdf), AAAI 2018

[Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols](https://arxiv.org/abs/1705.11192), NeurIPS 2017

[Emergent Translation in Multi-Agent Communication](https://arxiv.org/abs/1710.06922), ICLR 2018 [code](https://github.com/facebookresearch/translagent)

[Emergence of Linguistic Communication From Referential Games with Symbolic and Pixel Input](https://openreview.net/pdf?id=HJGv1Z-AW), ICLR 2018

[Emergent Communication through Negotiation](https://openreview.net/pdf?id=Hk6WhagRW), ICLR 2018 [code](https://github.com/ASAPPinc/emergent_comms_negotiation)

[On the Pitfalls of Measuring Emergent Communication](https://arxiv.org/abs/1903.05168), AAMAS 2019 [code](https://github.com/facebookresearch/measuring-emergent-comm)

[Emergence of Compositional Language with Deep Generational Transmission](https://arxiv.org/abs/1904.09067), ICML 2019

### Commonsense Reasoning

From Recognition to Cognition: Visual Commonsense Reasoning, CVPR 2018 [paper](https://arxiv.org/abs/1811.10830) [code](https://visualcommonsense.com/)

### Multimodal Reinforcement Learning

### Multimodal Dialog

### Language and Audio

### Video-based Activity Recognition

### Affect Recognition

Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph, ACL 2018 [paper](http://aclweb.org/anthology/P18-1208) [code](https://github.com/A2Zadeh/CMU-MultimodalSDK)

AMHUSE - A Multimodal dataset for HUmor SEnsing, ICMI 2017 [paper](https://dl.acm.org/citation.cfm?id=3136806) [code](http://amhuse.phuselab.di.unimi.it/)

### Healthcare

### Self-driving Cars

### Robotics

Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks [paper] (https://arxiv.org/abs/1810.10191)

# Workshops

The How2 Challenge: New Tasks for Vision & Language, ICML 2019 [link](https://srvk.github.io/how2-challenge/)

Visual Question Answering and Dialog, CVPR 2019, CVPR 2017 [link](https://visualqa.org/workshop.html)

Multi-modal Learning from Videos, CVPR 2019 [link](https://sites.google.com/view/mmlv/home)

Multimodal Learning and Applications Workshop, CVPR 2019, ECCV 2018 [link](https://mula-workshop.github.io/)

Habitat: Embodied Agents Challenge and Workshop, CVPR 2019 [link](https://aihabitat.org/workshop/)

Closing the Loop Between Vision and Language & LSMD Challenge, ICCV 2019 [link](https://sites.google.com/site/iccv19clvllsmdc/)

Multi-modal Video Analysis and Moments in Time Challenge, ICCV 2019 [link](https://sites.google.com/view/multimodalvideo/)

Cross-Modal Learning in Real World, ICCV 2019 [link](https://cromol.github.io/)

Spatial Language Understanding and Grounded Communication for Robotics, NAACL 2019 [link](https://splu-robonlp.github.io/)

YouTube-8M Large-Scale Video Understanding, ICCV 2019, ECCV 2018, CVPR 2017 [link](https://research.google.com/youtube8m/workshop2018/)

Language and Vision Workshop, CVPR 2019, CVPR 2018, CVPR 2017, CVPR 2015 [link](http://languageandvision.com/)

Sight and Sound, CVPR 2019, CVPR 2018 [link](http://sightsound.org/)

The Large Scale Movie Description Challenge (LSMDC), ICCV 2019, ICCV 2017 [link](https://sites.google.com/site/describingmovies/)

Visually Grounded Interaction and Language, NeurIPS 2018 [link](https://nips2018vigil.github.io/)

Wordplay: Reinforcement and Language Learning in Text-based Games, NeurIPS 2018 [link](https://www.wordplay2018.com/)

Interpretability and Robustness in Audio, Speech, and Language, NeurIPS 2018 [link](https://irasl.gitlab.io/)

Shortcomings in Vision and Language, ECCV 2018 [link](https://sites.google.com/view/sivl/)

Grand Challenge and Workshop on Human Multimodal Language, ACL 2018 [link](http://multicomp.cs.cmu.edu/acl2018multimodalchallenge/)

Computational Approaches to Subjectivity, Sentiment and Social Media Analysis: EMNLP 2018, EMNLP 2017, NAACL-HLT 2016, EMNLP 2015, ACL 2014, NAACL-HLT 2013 [link](https://wt-public.emm4u.eu/wassa2018/)

Visual Understanding Across Modalities, CVPR 2017 [link](http://vuchallenge.org/)

International Workshop on Computer Vision for Audio-Visual Media, ICCV 2017 [link](https://cvavm2017.wordpress.com/)

Language Grounding for Robotics, ACL 2017 [link](https://robo-nlp.github.io/2017_index.html)

Computer Vision for Audio-visual Media, ECCV 2016 [link](https://cvavm2016.wordpress.com/)

Language and Vision: ACL 2016, EMNLP 2015 [link](https://vision.cs.hacettepe.edu.tr/vl2016/)

# Tutorials
Connecting Language and Vision to Actions, ACL 2018 [link](https://lvatutorial.github.io/)

Multimodal Machine Learning, ACL 2017, CVPR 2016, ICMI 2016 [link](https://sites.google.com/site/multiml2016cvpr/)

Vision and Language: Bridging Vision and Language with Deep Learning, ICIP 2017 [link](https://www.microsoft.com/en-us/research/publication/vision-language-bridging-vision-language-deep-learning/)

# Courses
CMU 11-777, Advanced Multimodal Machine Learning [link](https://piazza.com/cmu/fall2018/11777/resources)

CMU 16-785, Integrated Intelligence in Robotics: Vision, Language, and Planning [link](http://www.cs.cmu.edu/~jeanoh/16-785/)

CMU 10-808, Language Grounding to Vision and Control [link](https://katefvision.github.io/LanguageGrounding/)

Virginia Tech CS 6501-004, Vision & Language [link](http://www.cs.virginia.edu/~vicente/vislang/)
